# Implementation Plan: Mika Taiko Chatbot

**Branch**: `1-mika-bot` | **Date**: 2026-01-08 | **Last Updated**: 2026-01-08 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `/specs/1-mika-bot/spec.md`

**Note**: This plan is generated by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

Develop a Taiko no Tatsujin themed QQ chatbot named "Mika" that responds to user queries with game-themed content, song information, and contextual conversations. The system uses a modular 5-step processing chain orchestrated by Temporal.io for reliability (with exponential backoff retry: 1s, 2s, 4s, 8s intervals, max 5 attempts), with FastAPI backend, Beanie ODM for MongoDB, LangBot for QQ integration, and gpt-4o via OpenRouter for AI responses. The architecture supports multi-group scalability, rate limiting (20/user/min, 50/group/min), privacy compliance (hashed user IDs, 90-day conversation retention), multi-modal content processing (10MB image limit, JPEG/PNG/WebP formats), structured JSON logging, basic monitoring metrics, in-memory caching with periodic refresh for song data, hybrid content filtering (keyword lists + LLM judgment), comprehensive testing infrastructure (automated pytest suite with fixtures/mocks, manual testing scripts per NFR-012), and continuous deployment architecture (all infrastructure services must run continuously per NFR-013: FastAPI backend + Temporal client, Temporal server + PostgreSQL, MongoDB, LangBot, Napcat, and Nginx recommended).

**Primary Technical Approach**: 
- Modular step-based processing (step1.py through step5.py) for maintainability and testability
- Temporal.io Workflow orchestration for retries, fault tolerance, and concurrency
- Beanie async ODM for efficient MongoDB operations
- FastAPI + Uvicorn for high-performance async API
- LangBot with keyword triggers and group whitelist for QQ integration
- OpenRouter API with gpt-4o for multi-modal AI responses

## Technical Context

**Language/Version**: Python 3.12+ (as per constitution)  
**Primary Dependencies**: 
- FastAPI 0.104+ (async web framework)
- Uvicorn 0.24+ (ASGI server)
- Beanie 1.23+ (async MongoDB ODM)
- Temporal Python SDK 1.5+ (workflow orchestration with exponential backoff retry)
- LangBot (QQ integration library, deployed as external service)
- OpenRouter API client (gpt-4o access with multi-modal support)
- Poetry 1.6+ (dependency management)
- ngrok or similar tunneling service (for local development webhook exposure)
- Pydantic 2.5+ (data validation)
- httpx (async HTTP client)
- python-jose (hashing for user IDs - SHA-256)
- langdetect (language detection)
- rapidfuzz (fuzzy matching for song names)
- structlog or python-json-logger (structured JSON logging)
- Jinja2 (optional, for prompt template engine per FR-013)

**Storage**: MongoDB 7.0+ (via Beanie ODM) for user data, conversation history, and impressions  
**Testing**: pytest 7.4+ with pytest-asyncio for async tests, coverage 80%+ target. Comprehensive testing infrastructure per NFR-012: (1) Automated test suite with unit tests (tests/unit/) and integration tests (tests/integration/) covering all user stories and edge cases, (2) Test utilities including pytest fixtures (tests/fixtures/) for common test data (mock users, conversations, songs) and mocks for external services (OpenRouter API, taikowiki, MongoDB), (3) Manual testing scripts in `scripts/` directory for quick functional verification (e.g., `scripts/test_webhook.py`, `scripts/test_song_query.py`)  
**Target Platform**: Linux server (Docker Compose deployment)  
**Project Type**: Single backend service (FastAPI application)  
**Performance Goals**: 
- Response time < 3 seconds for 95% of queries (NFR-001)
- Handle 100 concurrent requests without degradation (NFR-002)
- Rate limiting: 20 requests/user/minute, 50 requests/group/minute (FR-012)

**Constraints**: 
- < 3 seconds response time under normal load
- 99%+ uptime requirement (NFR-003)
- Privacy compliance: hashed user IDs (SHA-256), 90-day conversation retention (FR-011, FR-005)
- Budget constraint: < $0.01 per query for gpt-4o calls
- Mid-tier VPS: 4-core, 8GB RAM target
- API keys MUST be loaded from environment variables via config.py using os.getenv() (NFR-009)
- Configuration files MUST contain placeholder values only (leave API keys empty/blank) (NFR-009)
- Image processing limits: maximum 10MB, formats JPEG/PNG/WebP only (FR-006). For Taiko no Tatsujin related images, system MUST provide comprehensive detailed analysis including song identification, difficulty level, score details, and other relevant game elements. For non-Taiko related images, system MUST still provide a themed response but politely indicate that the bot focuses on Taiko-related content. All image analysis responses MUST maintain thematic consistency with game elements ("Don!", "Katsu!", emojis ü•Åüé∂) per FR-003
- Comprehensive prompt engineering required for detailed image analysis (FR-006)
- Prompt template system: Structured template system with simple API for adding prompts, support for versioning and A/B testing (FR-013). Prompts organized hierarchically: use_case (general_chat, song_query, image_analysis, memory_aware) ‚Üí intent (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) ‚Üí scenario (optional, for specific contexts like "song_recommendation_high_bpm", "difficulty_advice_beginner"). System MUST support fine-grained intent classification for more accurate and contextually appropriate responses. Intent detection can use rule-based keyword matching initially, with optional LLM-based classification for complex cases. Prompts stored in `src/prompts.py` or `src/prompts/` directory
- User preference extraction: System MUST use LLM to automatically analyze conversation content and extract user preferences. Before updating Impression with learned preferences, system MUST ask user for confirmation in bot's response. System MUST prioritize explicit confirmation (user replies with "ÊòØ", "ÂØπ", "yes", "correct", etc.). If explicit confirmation is not received, system MAY accept implicit confirmation if user continues conversation assuming preference is correct. Only after user confirms (explicitly or implicitly) should preferences be permanently stored (FR-010)
- Fuzzy matching strategy: When fuzzy matching finds multiple potential song matches, system MUST return only the single best match (highest similarity score) and ask user for confirmation in the response (e.g., "‰Ω†ÊòØÊåá„ÄäXXX„ÄãÂêóÔºü" / "Did you mean 'XXX'?") before providing song details. The confirmation request MUST be included in a themed response with game elements per FR-003 (FR-004)
- Conversation history limit: System MUST support configuration of conversation history limit via environment variable (e.g., `CONVERSATION_HISTORY_LIMIT`) with a default value of 10. This allows future flexibility to adjust context window size based on performance requirements, LLM token limits, or user needs. The configuration MUST be loaded from config.py using environment variables per NFR-009 (FR-005)
- Testing infrastructure: Comprehensive testing infrastructure per NFR-012 including automated pytest suite (unit tests, integration tests, 80%+ coverage target), test utilities (pytest fixtures for mock users/conversations/songs, mocks for external services), and manual testing scripts in `scripts/` directory (test_webhook.py, test_song_query.py)
- Deployment architecture: All infrastructure services MUST run continuously per NFR-013: (1) FastAPI backend + Temporal client (must handle webhooks, call gpt-4o, execute 5-step chain), (2) Temporal server + PostgreSQL (workflow engine must be online to schedule tasks and retries), (3) MongoDB (must store user history and impressions; downtime causes memory loss), (4) Nginx strongly recommended (long-running reverse proxy, HTTPS, load balancing for stable public network entry point). External services that must also run continuously: LangBot (core bot platform, deployed separately, must receive QQ messages and manage triggers, connects to FastAPI backend via webhook), Napcat (QQ protocol layer, deployed separately or as part of LangBot, must keep bot account online; downtime prevents message reception). Docker Compose deployment MUST ensure all services start and remain running with appropriate health checks and restart policies. Note: LangBot and Napcat are external services deployed separately from this project

**Scale/Scope**: 
- Multi-group deployment (dozens of QQ groups)
- 100+ concurrent users
- taikowiki API song database (PRIMARY data source) with local JSON file (data/database.json) as fallback
- Multi-modal image processing support

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

### I. Fun and Thematic Consistency ‚úÖ
- **Requirement**: All responses incorporate game elements ("Don!", "Katsu!", emojis ü•Åüé∂)
- **Implementation**: Structured prompt template system (`prompts.py` or `prompts/` directory) will contain themed prompt templates organized hierarchically by use case ‚Üí intent ‚Üí scenario; step4.py (LLM invocation) will inject Taiko knowledge using template system with intent-based prompt selection (FR-013). System supports fine-grained intent classification (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) and scenario-based prompts for specific contexts. Intent detection uses rule-based keyword matching initially, with optional LLM-based classification for complex cases
- **Verification**: All LLM prompts must include bot identity ("Mika" as drum spirit) and game terminology; prompt template system enables easy iteration and versioning; intent classification ensures contextually appropriate responses; scenario-based prompts provide specialized responses for specific contexts

### II. User Privacy and Safety ‚úÖ
- **Requirement**: Hashed QQ user IDs, minimal data retention, content filtering, secure credential management
- **Implementation**: 
  - step1.py: Hash QQ user IDs using SHA-256 before storage (FR-011)
  - step5.py: Implement 90-day auto-deletion for conversations (FR-005)
  - step1.py: Content filtering using hybrid approach - keyword/phrase lists for fast pre-filtering, then LLM judgment for ambiguous cases (FR-007)
  - config.py: All API keys loaded from environment variables via os.getenv(), configuration files contain placeholder values only (NFR-009)
  - Image processing: Enforce 10MB size limit and format restrictions (JPEG, PNG, WebP), return user-friendly themed error messages (e.g., "ÂõæÁâáÂ§™Â§ßÂï¶ÔºÅ") when limits exceeded (FR-006)
- **Verification**: No plaintext QQ IDs in database; automatic cleanup job for old conversations; no hard-coded secrets in codebase; content filtering accuracy tests; image validation tests; comprehensive test coverage (80%+ target) with automated and manual testing scripts

### III. Reliability and Scalability ‚úÖ
- **Requirement**: Temporal orchestration, < 3s response time, graceful degradation, structured logging, monitoring
- **Implementation**: 
  - Temporal Workflow coordinates all 5 steps as Activities with exponential backoff retry (1s, 2s, 4s, 8s intervals, max 5 attempts) (FR-009)
  - Retry policies for external service calls (taikowiki, OpenRouter)
  - Fallback mechanisms in step3.py (local JSON file when taikowiki API unavailable, in-memory cache with hourly refresh) and step4.py (default themed responses with user notification) (FR-002, FR-009)
  - Message deduplication for rapid consecutive messages from same user (FR-008)
  - Structured JSON logging with structured fields (user ID hashed, request ID, timestamp, operation type, log level, message, contextual metadata) (NFR-010)
  - Basic monitoring metrics: request rate, error rate, response time (p50, p95, p99), system resource usage (CPU, memory) via /health and /metrics endpoints (NFR-011)
- **Verification**: Load testing for 100 concurrent requests; error handling tests; log aggregation verification; metrics export validation; automated test suite with fixtures and mocks; manual testing scripts for rapid iteration; Docker Compose deployment with all services running continuously (health checks and restart policies configured)

### IV. Ethical AI Usage ‚úÖ
- **Requirement**: Culturally sensitive, fact-based responses, transparency about learning
- **Implementation**: 
  - Structured prompt template system (`prompts.py` or `prompts/` directory): Include cultural sensitivity guidelines, organized by use case with versioning support (FR-013)
  - step4.py: Inject taikowiki data for fact-based responses using template system
  - step5.py: Inform users when impressions are updated (FR-010). System MUST use LLM to automatically analyze conversation content and extract user preferences. Before updating Impression with learned preferences, system MUST ask user for confirmation in bot's response. System MUST prioritize explicit confirmation (user replies with "ÊòØ", "ÂØπ", "yes", "correct", etc.). If explicit confirmation is not received, system MAY accept implicit confirmation if user continues conversation assuming preference is correct. Only after user confirms (explicitly or implicitly) should preferences be permanently stored
- **Verification**: Prompt review for bias; user notification tests; preference confirmation flow tests (explicit and implicit confirmation scenarios); prompt versioning enables A/B testing for ethical improvements

**Gate Status**: ‚úÖ **PASS** - All constitution principles addressed in design

## Project Structure

### Documentation (this feature)

```text
specs/1-mika-bot/
‚îú‚îÄ‚îÄ plan.md              # This file (/speckit.plan command output)
‚îú‚îÄ‚îÄ research.md          # Phase 0 output (/speckit.plan command)
‚îú‚îÄ‚îÄ data-model.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ quickstart.md        # Phase 1 output (/speckit.plan command)
‚îú‚îÄ‚îÄ contracts/           # Phase 1 output (/speckit.plan command)
‚îÇ   ‚îú‚îÄ‚îÄ api.yaml         # OpenAPI specification
‚îÇ   ‚îî‚îÄ‚îÄ temporal.yaml    # Temporal workflow definitions
‚îî‚îÄ‚îÄ tasks.md             # Phase 2 output (/speckit.tasks command - NOT created by /speckit.plan)
```

### Source Code (repository root)

```text
taiko_bot/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ steps/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step1.py          # Input parsing + Mika name detection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step2.py          # Retrieve user context via Beanie
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step3.py          # taikowiki JSON song query with fuzzy matching
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step4.py          # Invoke gpt-4o via OpenRouter
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ step5.py          # Update user impression/history in Beanie
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py           # User model (hashed ID, preferences)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation.py   # Conversation model (90-day retention)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ impression.py     # Impression model (user memory)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ song.py           # Song model (from taikowiki)
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py       # Beanie initialization and connection
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py            # OpenRouter client for gpt-4o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ song_query.py     # taikowiki JSON query with fuzzy matching + in-memory cache with hourly refresh
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_filter.py # Content filtering (hybrid: keyword lists + LLM judgment)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rate_limiter.py  # Rate limiting (20/user/min, 50/group/min)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ intent_detection.py # Intent classification (rule-based + optional LLM-based)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py         # Structured JSON logging configuration
‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ message_workflow.py  # Temporal Workflow orchestrating steps 1-5
‚îÇ   ‚îú‚îÄ‚îÄ activities/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step1_activity.py    # Temporal Activity for step1
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step2_activity.py    # Temporal Activity for step2
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step3_activity.py    # Temporal Activity for step3
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ step4_activity.py    # Temporal Activity for step4
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ step5_activity.py    # Temporal Activity for step5
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py             # FastAPI application entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ langbot.py       # LangBot webhook endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ middleware/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ rate_limit.py   # Rate limiting middleware
‚îÇ   ‚îú‚îÄ‚îÄ config.py                # Configuration (env vars via os.getenv(), placeholder values only)
‚îÇ   ‚îú‚îÄ‚îÄ prompts.py               # Taiko-themed prompt templates with structured template system (FR-013: simple API, versioning, A/B testing support)
‚îÇ   ‚îÇ                            # Organized hierarchically: use_case ‚Üí intent ‚Üí scenario
‚îÇ   ‚îÇ                            # Supports fine-grained intents (greeting, help, goodbye, song_recommendation, difficulty_advice, etc.)
‚îÇ   ‚îÇ                            # OR src/prompts/ directory with individual template files organized by use case
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ hashing.py           # User ID hashing utilities
‚îÇ       ‚îî‚îÄ‚îÄ language_detection.py # Language detection utilities
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_step1.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_step2.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_step3.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_step4.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_step5.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_content_filter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_rate_limiter.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_workflow.py     # End-to-end workflow test
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api.py          # API integration tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_langbot_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ conftest.py          # pytest fixtures for test data and mocks
‚îÇ       ‚îú‚îÄ‚îÄ mock_users.py         # Mock user data fixtures
‚îÇ       ‚îú‚îÄ‚îÄ mock_conversations.py # Mock conversation data fixtures
‚îÇ       ‚îú‚îÄ‚îÄ mock_songs.py         # Mock song data fixtures
‚îÇ       ‚îú‚îÄ‚îÄ mock_llm.py           # Mock OpenRouter API responses
‚îÇ       ‚îú‚îÄ‚îÄ mock_taikowiki.py     # Mock taikowiki JSON responses
‚îÇ       ‚îú‚îÄ‚îÄ mock_mongodb.py       # Mock MongoDB fixtures
‚îÇ       ‚îú‚îÄ‚îÄ sample_messages.json
‚îÇ       ‚îî‚îÄ‚îÄ sample_songs.json
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ init_database.py          # Database initialization script
‚îÇ   ‚îú‚îÄ‚îÄ seed_songs.py             # Song data seeding script
‚îÇ   ‚îú‚îÄ‚îÄ test_webhook.py           # Manual testing script for LangBot webhook (NFR-012)
‚îÇ   ‚îî‚îÄ‚îÄ test_song_query.py        # Manual testing script for song queries (NFR-012)
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.backend
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.temporal
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml       # Full stack: backend, mongo, temporal, temporal-postgresql, nginx (all services must run continuously per NFR-013). Note: LangBot and Napcat are external services deployed separately
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ cleanup_old_conversations.py  # 90-day cleanup job
‚îÇ   ‚îî‚îÄ‚îÄ seed_songs.py                # Optional: seed song data
‚îú‚îÄ‚îÄ pyproject.toml                 # Poetry dependencies
‚îú‚îÄ‚îÄ poetry.lock                    # Locked dependencies
‚îú‚îÄ‚îÄ .env.example                   # Environment variable template
‚îî‚îÄ‚îÄ README.md
```

**Structure Decision**: Single backend project with modular step-based architecture. Steps are separated into individual files for maintainability and testability. Temporal Activities wrap each step for orchestration. FastAPI serves as the HTTP layer receiving LangBot webhooks.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation | Why Needed | Simpler Alternative Rejected Because |
|-----------|------------|-------------------------------------|
| N/A | All design choices align with constitution | N/A |

## Phase 0: Research & Technical Decisions

### Research Tasks

1. **LangBot Integration Patterns**
   - Research: LangBot keyword trigger configuration for "Mika" variants
   - Research: Group whitelist implementation in LangBot
   - Research: LangBot webhook format and message structure
   - Research: Webhook endpoint exposure methods (ngrok for local development, Nginx/public IP for production)
   - Decision: Use LangBot's keyword trigger with regex pattern for name variants. LangBot connects to FastAPI backend via webhook at `/webhook/langbot`. For local development, use ngrok or similar tunneling service to expose webhook endpoint. For production, use public domain/IP with Nginx reverse proxy (HTTPS recommended)
   - Rationale: LangBot provides built-in keyword matching; group whitelist prevents unauthorized access. Webhook-based integration allows LangBot (external service) to communicate with FastAPI backend. Tunneling services enable local development without public infrastructure

2. **Temporal.io Workflow Patterns**
   - Research: Temporal Python SDK Activity patterns for async operations
   - Research: Retry policies for external API calls (OpenRouter, taikowiki)
   - Research: Temporal concurrency limits for multi-group scenarios
   - Decision: Use Temporal Activities for each step with exponential backoff retries
   - Rationale: Temporal provides built-in retry, timeout, and fault tolerance; Activities are idempotent

3. **Beanie ODM Best Practices**
   - Research: Beanie async query patterns for user context retrieval
   - Research: Beanie bulk operations for conversation cleanup
   - Research: Beanie indexing strategies for hashed user IDs
   - Decision: Use Beanie Document models with async queries; create index on hashed_user_id
   - Rationale: Beanie provides type-safe async MongoDB operations; indexing improves query performance

4. **OpenRouter API Integration**
   - Research: OpenRouter API format for gpt-4o requests
   - Research: Multi-modal (vision) support in OpenRouter
   - Research: Cost optimization strategies (token limits, caching)
   - Decision: Use OpenRouter REST API with vision support; implement response caching
   - Rationale: OpenRouter provides unified API for multiple models; caching reduces costs

5. **taikowiki API Structure and Data Source Strategy**
   - Research: taikowiki API schema and song data format
   - Research: Real-time query patterns (caching vs. live fetch)
   - Research: Fuzzy matching algorithms for song names
   - Research: Primary vs. fallback data source patterns
   - Decision: taikowiki API is the PRIMARY data source. System MUST fetch from taikowiki API first and update local JSON file (data/database.json) periodically (e.g., hourly) for caching. Local JSON file is used ONLY as fallback when taikowiki API is unavailable. Use rapidfuzz library for fuzzy matching; implement in-memory caching with periodic refresh (e.g., hourly) for song data. Queries prioritize cached data for fast response times, with automatic background refresh to maintain data freshness. When updating local file, system SHOULD replace existing data to ensure consistency with API source (FR-002)
   - Rationale: Using API as primary source ensures data freshness and accuracy. Local file as fallback provides resilience during API outages. rapidfuzz provides fast fuzzy string matching. In-memory caching with periodic refresh balances performance optimization with data currency requirements. Replacing data on update maintains consistency

6. **Content Filtering Implementation**
   - Research: Content filtering libraries for Chinese text (hatred, politics, religion)
   - Research: Integration with LLM-based filtering vs. keyword-based
   - Decision: Use hybrid approach - keyword/phrase lists for fast pre-filtering of obviously inappropriate content, followed by LLM judgment for ambiguous cases (FR-007)
   - Rationale: Keyword filtering is fast and cost-efficient; LLM provides nuanced detection for edge cases. This balances filtering accuracy, cost efficiency, and performance while minimizing false positives

7. **Rate Limiting Architecture**
   - Research: Rate limiting patterns for async FastAPI applications
   - Research: Distributed rate limiting (Redis) vs. in-memory
   - Decision: Use in-memory rate limiting with sliding window for MVP; Redis for scale
   - Rationale: In-memory is simpler for MVP; Redis enables distributed rate limiting across instances

8. **Language Detection**
   - Research: Language detection libraries for Chinese/English
   - Research: User preference storage and retrieval
   - Decision: Use langdetect library; store preference in User model. System MUST automatically detect user message language, but MUST also allow users to explicitly specify their preferred language (NFR-008)
   - Rationale: langdetect is lightweight and accurate for Chinese/English; preference storage enables persistence; automatic detection with user override balances automation with user control for better UX

9. **Structured Logging and Monitoring**
   - Research: Structured logging libraries for Python (JSON format)
   - Research: Monitoring metrics collection and export patterns
   - Decision: Implement structured JSON logging with fields (user ID hashed, request ID, timestamp, operation type, log level, message, contextual metadata). Implement basic monitoring metrics (request rate, error rate, response time percentiles, system resource usage) accessible via /health and /metrics endpoints (NFR-010, NFR-011)
   - Rationale: Structured JSON logs support aggregation and querying for debugging, performance monitoring, and audit purposes. Basic monitoring metrics provide essential operational visibility without the complexity of full distributed tracing

10. **Temporal Workflow Retry Strategy**
    - Research: Temporal retry policy patterns and best practices
    - Research: Exponential backoff vs. fixed interval retry strategies
    - Decision: Implement exponential backoff retry strategy (e.g., 1s, 2s, 4s, 8s intervals) with a maximum of 5 retry attempts for transient failures (FR-009)
    - Rationale: Exponential backoff provides resilience against transient failures while preventing excessive retry overhead and ensures eventual consistency

11. **Image Processing Limits and Error Handling**
    - Research: Image size and format validation patterns
    - Research: User-friendly error message patterns for multi-modal content
    - Decision: Enforce reasonable image size limits (e.g., maximum 10MB) and format restrictions (JPEG, PNG, WebP). Return user-friendly, themed error messages (e.g., "ÂõæÁâáÂ§™Â§ßÂï¶ÔºÅ" with thematic elements) when limits exceeded. Implement comprehensive prompt engineering for detailed image analysis and themed response generation (FR-006)
    - Rationale: Reasonable limits balance user experience with system resource constraints. Themed error messages maintain consistency with bot identity. Comprehensive prompt engineering ensures high-quality image analysis responses

12. **Prompt Template System Architecture**
    - Research: Prompt template organization patterns (single file vs. directory structure)
    - Research: Template engine options (Python string templates vs. Jinja2) for variable substitution
    - Research: Prompt versioning and A/B testing patterns for LLM applications
    - Research: Intent classification and scenario-based prompt organization patterns
    - Decision: Implement structured prompt template system using Python string templates or Jinja2 with clear separation of concerns. Organize prompts hierarchically: use_case (general_chat, song_query, image_analysis, memory_aware) ‚Üí intent (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) ‚Üí scenario (optional, for specific contexts like "song_recommendation_high_bpm", "difficulty_advice_beginner"). Store prompts in single `src/prompts.py` file or `src/prompts/` directory. Provide simple API for adding new prompts (e.g., `add_prompt(name, template, variables, use_case, intent, scenario)` function) and support prompt versioning and A/B testing capabilities (FR-013). Implement intent detection service (`src/services/intent_detection.py`) using rule-based keyword matching initially, with optional LLM-based classification for complex cases
    - Rationale: Structured template system enables easy prompt iteration without code changes, facilitates prompt engineering best practices, and supports experimentation through versioning and A/B testing. Hierarchical organization (use_case ‚Üí intent ‚Üí scenario) enables precise prompt selection and better response quality. Fine-grained intent classification allows contextually appropriate responses for different user intents. Scenario-based prompts provide highly specialized responses for specific contexts. Simple API reduces barrier to adding new prompts. Rule-based detection provides fast, deterministic intent classification, while LLM-based classification handles ambiguous cases

13. **Testing Infrastructure Architecture** (NFR-012)
    - Research: pytest fixture patterns for async FastAPI applications
    - Research: Mocking strategies for external services (OpenRouter API, taikowiki, MongoDB)
    - Research: Test data management patterns (fixtures vs. factories)
    - Research: Manual testing script patterns for rapid iteration
    - Decision: Implement comprehensive testing infrastructure including: (1) Automated test suite using pytest with unit tests (tests/unit/) and integration tests (tests/integration/) covering all user stories and edge cases with 80%+ coverage target, (2) Test utilities including pytest fixtures (tests/fixtures/) for common test data (mock users, conversations, songs) and mocks for external services (OpenRouter API, taikowiki, MongoDB), (3) Manual testing scripts in `scripts/` directory for quick functional verification (e.g., `scripts/test_webhook.py` for testing LangBot webhook endpoint, `scripts/test_song_query.py` for testing song queries)
    - Rationale: Automated test suite ensures code quality and prevents regressions. Test utilities (fixtures and mocks) improve test maintainability and reduce duplication. Manual testing scripts enable rapid iteration and debugging during development. This provides both automated quality assurance and developer-friendly manual testing tools

14. **Deployment Architecture and Service Requirements** (NFR-013)
    - Research: Docker Compose service dependencies and health check patterns
    - Research: Service restart policies and high availability patterns
    - Research: Infrastructure service requirements for continuous operation
    - Research: Webhook endpoint exposure methods for external service integration (ngrok for local development, Nginx/public IP for production)
    - Decision: All infrastructure services MUST run continuously: (1) FastAPI backend + Temporal client (must handle webhooks, call gpt-4o, execute 5-step chain), (2) Temporal server + PostgreSQL (workflow engine must be online to schedule tasks and retries), (3) MongoDB (must store user history and impressions; downtime causes memory loss), (4) Nginx strongly recommended (long-running reverse proxy, HTTPS, load balancing for stable public network entry point). External services that must also run continuously: LangBot (core bot platform, deployed separately, must receive QQ messages and manage triggers, connects to FastAPI backend via webhook at `/webhook/langbot`), Napcat (QQ protocol layer, deployed separately or as part of LangBot, must keep bot account online; downtime prevents message reception). For local development, webhook endpoint must be exposed using tunneling service (ngrok recommended). For production, use public domain/IP with Nginx reverse proxy (HTTPS recommended). Docker Compose deployment MUST ensure all services start and remain running with appropriate health checks and restart policies. Note: LangBot and Napcat are external services deployed separately from this project's Docker Compose stack
    - Rationale: All listed services are critical for system functionality. Any service downtime will cause functional failures (webhook processing, workflow execution, data persistence, message reception). Health checks and restart policies ensure automatic recovery from transient failures and maintain system availability. Tunneling services enable local development and testing without requiring public infrastructure, while production deployment uses stable public endpoints

15. **User Preference Extraction and Confirmation Mechanism** (FR-010)
    - Research: LLM-based preference extraction patterns from conversation content
    - Research: Explicit vs. implicit user confirmation patterns in conversational AI
    - Research: Natural language confirmation detection (detecting "ÊòØ", "ÂØπ", "yes", "correct", etc.)
    - Research: Pending state management for unconfirmed preferences
    - Decision: System MUST use LLM to automatically analyze conversation content and extract user preferences (e.g., favorite song types, BPM preferences). Before updating Impression with learned preferences, system MUST ask user for confirmation in bot's response (e.g., "‰Ω†Â•ΩÂÉèÂñúÊ¨¢È´ò BPM Ê≠åÊõ≤ÔºåÂØπÂêóÔºü" / "It seems you like high-BPM songs, right?"). System MUST prioritize explicit confirmation (user replies with "ÊòØ", "ÂØπ", "yes", "correct", etc.). If explicit confirmation is not received, system MAY accept implicit confirmation if user continues conversation assuming preference is correct (e.g., continues discussing high-BPM songs after being asked about the preference). Only after user confirms (explicitly or implicitly) should preferences be permanently stored in the Impression model. If users do not respond to confirmation requests, system MUST retain unconfirmed preferences in pending state but MUST NOT actively re-ask the same question. System MUST wait for users to mention related topics in future conversations, then re-confirm the preference naturally in context
    - Rationale: Automatic extraction balances automation with accuracy. Asking for confirmation ensures user awareness and prevents incorrect preference storage. Prioritizing explicit confirmation provides higher accuracy, while implicit confirmation improves user experience by not requiring explicit responses. Not re-asking prevents user annoyance while preserving data collection opportunities. Natural context-based re-confirmation feels more conversational. This aligns with FR-010 (inform users when bot learns information) and balances automation with user control

16. **Fuzzy Matching Confirmation Strategy** (FR-004)
    - Research: Fuzzy matching result presentation patterns (single match vs. multiple matches)
    - Research: User confirmation patterns for ambiguous queries
    - Decision: When fuzzy matching finds multiple potential song matches, system MUST return only the single best match (highest similarity score) and ask user for confirmation in the response (e.g., "‰Ω†ÊòØÊåá„ÄäXXX„ÄãÂêóÔºü" / "Did you mean 'XXX'?"). The confirmation request MUST be included in a themed response with game elements per FR-003. If the user confirms or continues the conversation assuming the match is correct, the system proceeds with that song. If the user indicates it's not the correct song, the system can offer alternative matches or ask for clarification
    - Rationale: Returning only the best match prevents information overload while ensuring accuracy. Asking for confirmation reduces errors from incorrect matches. Themed confirmation requests maintain consistency with bot identity. This balances user experience with accuracy

17. **Conversation History Limit Configuration** (FR-005)
    - Research: Configurable context window patterns for LLM applications
    - Research: Environment variable configuration patterns for context limits
    - Decision: System MUST support configuration of conversation history limit via environment variable (e.g., `CONVERSATION_HISTORY_LIMIT`) with a default value of 10. This allows future flexibility to adjust context window size based on performance requirements, LLM token limits, or user needs. The configuration MUST be loaded from config.py using environment variables per NFR-009
    - Rationale: Configurable limit enables future optimization without code changes. Default value of 10 provides reasonable context while maintaining performance. Environment variable configuration aligns with NFR-009 (no hard-coded values) and allows deployment-specific tuning

18. **Intent Classification and Scenario-Based Prompting** (FR-013 Enhancement)
    - Research: Intent classification patterns for conversational AI (greeting, help, goodbye, preference_confirmation, etc.)
    - Research: Scenario-based prompt organization (song_recommendation, difficulty_advice, game_tips, etc.)
    - Research: Intent detection methods (rule-based keywords vs. LLM-based classification)
    - Research: Multi-level prompt hierarchy (use_case ‚Üí intent ‚Üí scenario)
    - Decision: System MUST support fine-grained intent classification within use cases. Intent categories include: (1) **Conversational Intents**: greeting, help, goodbye, preference_confirmation, clarification_request, (2) **Song-Related Intents**: song_query, song_recommendation, difficulty_advice, bpm_analysis, (3) **Game-Related Intents**: game_tips, achievement_celebration, practice_advice. Each intent can have its own specialized prompt template. System MUST also support scenario-based prompts for specific contexts (e.g., "song_recommendation_high_bpm", "difficulty_advice_beginner"). Intent detection can be implemented using rule-based keyword matching initially, with optional LLM-based classification for complex cases. Prompts are organized hierarchically: use_case (general_chat, song_query, image_analysis, memory_aware) ‚Üí intent (greeting, help, etc.) ‚Üí scenario (optional, for specific contexts). When intent detection fails or is uncertain, system MUST fallback to use_case-based prompt selection (infer use_case from message content) and MUST log intent detection failures for analysis and improvement. This enables precise prompt selection and better response quality
    - Rationale: Fine-grained intent classification enables more accurate and contextually appropriate responses. Different intents require different prompt structures and information. Scenario-based prompts allow for highly specialized responses in specific contexts. Hierarchical organization maintains clarity while supporting extensibility. Rule-based detection provides fast, deterministic intent classification, while LLM-based classification handles ambiguous cases. Fallback to use_case ensures functionality even when intent detection fails

19. **Song Database Primary Data Source and Fallback Strategy** (FR-002 Enhancement)
    - Research: Primary vs. fallback data source patterns for external APIs
    - Research: Local cache update strategies (replace vs. merge)
    - Decision: taikowiki API is the PRIMARY data source. System MUST fetch from taikowiki API first and update local JSON file (data/database.json) periodically (e.g., hourly) for caching. Local JSON file is used ONLY as fallback when taikowiki API is unavailable. When updating local file, system SHOULD replace existing data to ensure consistency with API source. This ensures data freshness while providing resilience through local cache
    - Rationale: Using API as primary source ensures data freshness and accuracy. Local file as fallback provides resilience during API outages. Replacing data on update maintains consistency and avoids merge conflicts

20. **Concurrent Message Deduplication Strategy** (FR-008 Enhancement)
    - Research: Message deduplication patterns for chat applications
    - Research: Similarity threshold and time window configuration patterns
    - Decision: When the same user sends rapid consecutive messages, system MUST process messages in order but MUST skip duplicate or highly similar messages (deduplication). Similarity threshold and deduplication window (e.g., within 5 seconds) MUST be configurable via environment variables. This ensures message order is preserved while avoiding redundant processing of duplicate content, improving efficiency and reducing costs
    - Rationale: Deduplication prevents redundant processing of duplicate messages, reducing system load and costs. Configurable thresholds allow tuning based on use case. Preserving message order maintains user experience

21. **Unconfirmed Preference Handling Strategy** (FR-010 Enhancement)
    - Research: Pending state management patterns for user confirmations
    - Research: Natural context-based re-confirmation patterns
    - Decision: If users do not respond to preference confirmation requests, system MUST retain unconfirmed preferences in pending state but MUST NOT actively re-ask the same question. System MUST wait for users to mention related topics in future conversations, then re-confirm the preference naturally in context. This avoids annoying users with repeated questions while preserving the opportunity for natural confirmation when relevant
    - Rationale: Not re-asking prevents user annoyance while preserving data collection opportunities. Natural context-based re-confirmation feels more conversational and less intrusive

22. **Detailed Error Recovery and Fallback Strategies** (FR-009 Enhancement)
    - Research: Graceful degradation patterns for LLM and external API failures
    - Research: User notification patterns for degraded service states
    - Decision: When external services (LLM, song database) fail after retries, system MUST use cached data or default themed responses and MUST notify users in the response (e.g., "‰ΩøÁî®ÁºìÂ≠òÊï∞ÊçÆÔºåÂèØËÉΩ‰∏çÊòØÊúÄÊñ∞ÁöÑ" / "Using cached data, may not be latest"). Song database fallback: when taikowiki API is unavailable, system MUST use local JSON file (data/database.json) as fallback. LLM fallback: system may support local LLM models in the future as an alternative to OpenRouter API. This provides graceful degradation while maintaining functionality
    - Rationale: Transparent user notification maintains trust and sets expectations. Cached data and default responses ensure functionality during outages. Local fallbacks provide additional resilience layers

**Output**: `research.md` with all technical decisions documented

**Key Clarifications Integrated**:
- ‚úÖ Structured prompt template system with simple API, versioning, and A/B testing support (FR-013)
- ‚úÖ Structured JSON logging with aggregation support (NFR-010)
- ‚úÖ Basic monitoring metrics with /health and /metrics endpoints (NFR-011)
- ‚úÖ Hybrid content filtering (keyword lists + LLM judgment) (FR-007)
- ‚úÖ In-memory caching with periodic refresh for song data (FR-002)
- ‚úÖ Exponential backoff retry strategy for Temporal workflows (FR-009)
- ‚úÖ Image processing limits (10MB, JPEG/PNG/WebP) with themed error messages (FR-006). For Taiko no Tatsujin related images, comprehensive detailed analysis including song identification, difficulty level, score details, and other relevant game elements. For non-Taiko related images, themed response but politely indicate bot focuses on Taiko-related content. All image analysis responses maintain thematic consistency with game elements ("Don!", "Katsu!", emojis ü•Åüé∂) per FR-003
- ‚úÖ API key management via environment variables, config files with placeholder values only (NFR-009)
- ‚úÖ Comprehensive prompt engineering for image analysis (FR-006)
- ‚úÖ User preference extraction with confirmation: LLM automatically analyzes conversation content and extracts preferences. Before updating Impression, system asks user for confirmation. System prioritizes explicit confirmation ("ÊòØ", "ÂØπ", "yes", "correct", etc.), with implicit confirmation as fallback if user continues conversation assuming preference is correct. Only after user confirms (explicitly or implicitly) should preferences be permanently stored (FR-010)
- ‚úÖ Fuzzy matching confirmation strategy: When multiple matches found, return only single best match (highest similarity score) and ask user for confirmation (e.g., "‰Ω†ÊòØÊåá„ÄäXXX„ÄãÂêóÔºü" / "Did you mean 'XXX'?") before providing song details. Confirmation request included in themed response with game elements per FR-003 (FR-004)
- ‚úÖ Conversation history limit configuration: Support configuration via environment variable (e.g., `CONVERSATION_HISTORY_LIMIT`) with default value of 10. Allows future flexibility to adjust context window size based on performance requirements, LLM token limits, or user needs. Configuration loaded from config.py using environment variables per NFR-009 (FR-005)
- ‚úÖ Intent classification and scenario-based prompting: System supports fine-grained intent classification (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) and scenario-based prompts for specific contexts (e.g., "song_recommendation_high_bpm", "difficulty_advice_beginner"). Prompts organized hierarchically: use_case ‚Üí intent ‚Üí scenario. Intent detection uses rule-based keyword matching initially, with optional LLM-based classification for complex cases. When intent detection fails, system falls back to use_case-based prompt selection and logs failures for improvement. This enables precise prompt selection and better response quality (FR-013 Enhancement)
- ‚úÖ Song database primary data source: taikowiki API is PRIMARY data source, local JSON file (data/database.json) used ONLY as fallback when API unavailable. Local file updated periodically (hourly) by replacing existing data for consistency (FR-002 Enhancement)
- ‚úÖ Concurrent message deduplication: System processes messages in order but skips duplicate or highly similar messages. Similarity threshold and deduplication window (e.g., 5 seconds) configurable via environment variables (FR-008 Enhancement)
- ‚úÖ Unconfirmed preference handling: System retains unconfirmed preferences in pending state but does not actively re-ask. Re-confirms naturally when users mention related topics in future conversations (FR-010 Enhancement)
- ‚úÖ Detailed error recovery: System uses cached data or default themed responses with user notification when services fail. Song database falls back to local JSON file, LLM may support local models in future (FR-009 Enhancement)
- ‚úÖ Comprehensive testing infrastructure (automated pytest suite, fixtures/mocks, manual testing scripts) (NFR-012)
- ‚úÖ Deployment architecture with continuous service requirements (all infrastructure services must run continuously) (NFR-013)

## Phase 1: Design & Contracts

### Data Model Design

**Entities** (detailed in `data-model.md`):

1. **User** (hashed_user_id: str, preferences: dict, language: str, created_at: datetime, updated_at: datetime)
2. **Conversation** (user_id: str, message: str, response: str, timestamp: datetime, group_id: str, expires_at: datetime)
3. **Impression** (user_id: str, preferences: dict, relationship_status: str, interaction_count: int, last_interaction: datetime)
4. **Song** (name: str, difficulty: int, bpm: int, metadata: dict) - from taikowiki, cached

**Relationships**:
- User 1:1 Impression
- User 1:N Conversation (with 90-day auto-deletion)

### API Contracts

**OpenAPI Specification** (`contracts/api.yaml`):

```yaml
paths:
  /webhook/langbot:
    post:
      summary: LangBot webhook endpoint
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                group_id: {type: string}
                user_id: {type: string}
                message: {type: string}
                images: {type: array, items: {type: string, format: base64, maxLength: 10485760}}
      responses:
        200:
          description: Message processed successfully
        400:
          description: Invalid image format or size exceeded (10MB limit)
        429:
          description: Rate limit exceeded (20/user/min, 50/group/min)
  /health:
    get:
      summary: Health check endpoint
      responses:
        200:
          description: System is healthy
  /metrics:
    get:
      summary: Prometheus metrics export endpoint
      responses:
        200:
          description: Metrics in Prometheus format
```

**Temporal Workflow Definition** (`contracts/temporal.yaml`):

```yaml
workflows:
  process_message:
    activities:
      - step1_parse_input
      - step2_retrieve_context
      - step3_query_song
      - step4_invoke_llm
      - step5_update_impression
    retry_policy:
      initial_interval: 1s
      backoff_coefficient: 2.0
      maximum_interval: 8s
      maximum_attempts: 5
      # Exponential backoff: 1s, 2s, 4s, 8s intervals (FR-009)
```

### Quickstart Guide

**Output**: `quickstart.md` with:
- Local development setup (Poetry, MongoDB, Temporal)
- Environment variable configuration
- Running tests
- Docker Compose deployment
- LangBot configuration

### Agent Context Update

Run `.specify/scripts/powershell/update-agent-context.ps1 -AgentType cursor-agent` to update Cursor agent context with new technologies (Temporal, Beanie, LangBot).

**Output**: Updated `.cursor/context.md` or equivalent agent context file

## Development Order & Milestones

### Suggested Development Order

1. **Phase 1: Core Step Functions** (Week 1-2)
   - Implement step1.py (input parsing, name detection, hybrid content filtering: keyword lists + LLM judgment)
   - Implement step2.py (Beanie user context retrieval with configurable conversation history limit via `CONVERSATION_HISTORY_LIMIT` environment variable, default 10)
   - Implement step3.py (taikowiki API query as PRIMARY data source with fuzzy matching + in-memory caching with hourly refresh). Use local JSON file (data/database.json) ONLY as fallback when API unavailable. When updating local file, replace existing data for consistency. When multiple matches found, return only single best match (highest similarity score) and ask user for confirmation (e.g., "‰Ω†ÊòØÊåá„ÄäXXX„ÄãÂêóÔºü" / "Did you mean 'XXX'?") before providing song details. Confirmation request included in themed response with game elements per FR-003
   - Implement structured prompt template system in prompts.py (FR-013: simple API, versioning, A/B testing support) with hierarchical organization: use_case ‚Üí intent ‚Üí scenario. Support fine-grained intent classification (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) and scenario-based prompts for specific contexts
   - Implement intent detection in step1.py or new intent_detection.py service (rule-based keyword matching for common intents, optional LLM-based classification for complex cases)
   - Implement step4.py (OpenRouter gpt-4o invocation with comprehensive prompt engineering using template system). Select prompt based on detected intent and scenario. For Taiko no Tatsujin related images, provide comprehensive detailed analysis including song identification, difficulty level, score details, and other relevant game elements. For non-Taiko related images, provide themed response but politely indicate bot focuses on Taiko-related content. All image analysis responses maintain thematic consistency with game elements ("Don!", "Katsu!", emojis ü•Åüé∂) per FR-003
   - Implement step5.py (Beanie impression update). Implement user preference extraction: LLM automatically analyzes conversation content and extracts preferences. Before updating Impression, ask user for confirmation in bot's response. Prioritize explicit confirmation ("ÊòØ", "ÂØπ", "yes", "correct", etc.), with implicit confirmation as fallback if user continues conversation assuming preference is correct. Only after user confirms (explicitly or implicitly) should preferences be permanently stored. If users do not respond to confirmation requests, retain unconfirmed preferences in pending state but do not actively re-ask. Re-confirm naturally when users mention related topics in future conversations (FR-010)
   - Implement image processing validation (10MB limit, JPEG/PNG/WebP formats, themed error messages)
   - Implement structured JSON logging (logger.py service)
   - Implement message deduplication in step1.py or new deduplication service: process messages in order but skip duplicate or highly similar messages. Similarity threshold and deduplication window (e.g., 5 seconds) configurable via environment variables (FR-008)
   - **Milestone**: All steps independently testable with logging, caching, structured prompt system, fuzzy matching confirmation, preference extraction with confirmation, configurable conversation history limit, message deduplication, and error recovery with fallbacks

2. **Phase 2: Temporal Integration** (Week 2-3)
   - Create Temporal Activities for each step
   - Create Temporal Workflow orchestrating steps
   - Implement exponential backoff retry policies (1s, 2s, 4s, 8s intervals, max 5 attempts) and error handling
   - **Milestone**: End-to-end workflow testable with retry resilience

3. **Phase 3: FastAPI Backend** (Week 3-4)
   - Create FastAPI application
   - Implement LangBot webhook endpoint
   - Add rate limiting middleware (20/user/min, 50/group/min)
   - Implement /health and /metrics endpoints for monitoring
   - Configure structured JSON logging throughout application
   - **Milestone**: API accepts LangBot messages with monitoring and logging

4. **Phase 4: LangBot Integration** (Week 4)
   - Configure LangBot keyword triggers
   - Set up group whitelist
   - Expose webhook endpoint for LangBot connection (local development: use ngrok or similar tunneling service; production: use public domain/IP with Nginx)
   - Configure LangBot to send webhooks to FastAPI backend endpoint (`/webhook/langbot`)
   - Test end-to-end with QQ groups
   - **Milestone**: Bot responds in QQ groups via LangBot webhook connection

5. **Phase 5: Docker Deployment** (Week 5)
   - Create Dockerfiles for all services (backend, temporal worker)
   - Create docker-compose.yml with required services: backend, mongo, temporal, temporal-postgresql, nginx (recommended)
   - Configure health checks and restart policies for all services (per NFR-013: all services must run continuously)
   - Set up environment configuration
   - Document external service requirements (LangBot and Napcat must be deployed separately and configured to connect to FastAPI backend webhook)
   - **Milestone**: Full stack deployable via Docker Compose with all services running continuously. External services (LangBot, Napcat) documented and configured separately

6. **Phase 6: Testing Infrastructure** (Week 5-6, parallel with Phase 5)
   - Create pytest fixtures for common test data (mock users, conversations, songs) in tests/fixtures/
   - Create mocks for external services (OpenRouter API, taikowiki, MongoDB) in tests/fixtures/
   - Implement unit tests for all step functions (tests/unit/test_step*.py), including:
     - Fuzzy matching confirmation flow (single best match + user confirmation)
     - User preference extraction and confirmation (explicit and implicit confirmation scenarios)
     - Conversation history limit configuration (environment variable, default value)
     - Image analysis detail (Taiko vs. non-Taiko image handling)
     - Intent detection accuracy (rule-based keyword matching, LLM-based classification for complex cases)
     - Intent-based prompt selection logic
   - Implement integration tests for end-to-end workflows (tests/integration/), including preference extraction confirmation flows and intent-based response generation
   - Create manual testing scripts in scripts/ directory (test_webhook.py, test_song_query.py)
   - Achieve 80%+ test coverage target
   - **Milestone**: Comprehensive testing infrastructure complete per NFR-012 (automated tests, fixtures, mocks, manual scripts) with coverage for all new clarification features

7. **Phase 7: Intent Classification & Scenario-Based Prompting Enhancement** (Week 6)
   - Implement fine-grained intent classification system in src/services/intent_detection.py:
     - Conversational intents: greeting, help, goodbye, preference_confirmation, clarification_request
     - Song-related intents: song_query, song_recommendation, difficulty_advice, bpm_analysis
     - Game-related intents: game_tips, achievement_celebration, practice_advice
     - Rule-based keyword matching for common intents
     - Optional LLM-based classification for complex/ambiguous cases
   - Add intent-specific prompt templates to src/prompts.py:
     - greeting, help, goodbye prompts for conversational intents
     - song_recommendation, difficulty_advice, bpm_analysis prompts for song-related intents
     - game_tips, achievement_celebration, practice_advice prompts for game-related intents
   - Add scenario-based prompt templates for specific contexts:
     - song_recommendation_high_bpm, song_recommendation_beginner_friendly
     - difficulty_advice_beginner, difficulty_advice_expert
     - game_tips_timing, game_tips_accuracy
   - Update step4.py to use intent-based prompt selection:
     - Detect intent from parsed_input and context
     - Select appropriate prompt template based on intent and scenario
     - Fallback to use_case-based prompts if intent not detected (infer use_case from message content)
     - Log intent detection failures for analysis and improvement
   - Update step1.py or create intent_detection service to detect intents from user messages
   - **Milestone**: Intent classification system operational, contextually appropriate responses based on detected intents and scenarios

8. **Phase 8: Polish & Optimization** (Week 6-7)
   - Implement 90-day conversation cleanup job
   - Optimize caching strategies (verify hourly refresh mechanism)
   - Performance tuning
   - Verify API key management (environment variables, placeholder configs)
   - Verify comprehensive prompt engineering for image analysis
   - Verify intent classification accuracy and prompt selection logic
   - Verify error recovery strategies (cached data, local JSON fallback, user notifications)
   - Verify message deduplication functionality
   - Verify unconfirmed preference handling (pending state, natural re-confirmation)
   - **Milestone**: Production-ready system with all clarifications, intent classification, error recovery, and message deduplication implemented

### Key Milestones

- **M1**: Core step functions complete and tested (Week 2)
- **M1.5**: Testing infrastructure foundation (fixtures, mocks) ready for use (Week 5)
- **M2**: Temporal workflow operational (Week 3)
- **M3**: FastAPI + LangBot integration working (Week 4)
- **M4**: Multi-group deployment successful (Week 5)
- **M5**: Production deployment with monitoring (Week 6)

## Risks & Mitigation

### High-Risk Items

1. **QQ Account Ban Risk**
   - **Risk**: Bot behavior may trigger QQ platform bans
   - **Mitigation**: 
     - Strict rate limiting (20/user/min, 50/group/min)
     - Content filtering to avoid policy violations
     - Monitor account status; implement circuit breaker
   - **Contingency**: Use backup QQ account; implement graceful degradation

2. **OpenRouter Cost Overrun**
   - **Risk**: gpt-4o API costs may exceed budget (< $0.01/query)
   - **Mitigation**:
     - Implement response caching for common queries
     - Set token limits in prompts
     - Monitor API usage with alerts
   - **Contingency**: Fallback to cheaper model (gpt-3.5-turbo) for non-critical queries

3. **Temporal Complexity**
   - **Risk**: Temporal learning curve may delay development
   - **Mitigation**:
     - Start with simple workflow patterns
     - Use Temporal documentation and examples
     - Implement step functions first, then wrap in Activities
     - Configure exponential backoff retry (1s, 2s, 4s, 8s, max 5 attempts) from start (FR-009)
   - **Contingency**: Use async task queue (Celery) as fallback

4. **taikowiki Availability**
   - **Risk**: External taikowiki JSON may be unavailable
   - **Mitigation**:
     - Implement in-memory caching with periodic refresh (e.g., hourly) (FR-002)
     - Queries prioritize cached data for fast response times
     - Automatic background refresh to maintain data freshness
     - Fallback to cached data on failure
     - Graceful degradation (inform user, continue without song data)
   - **Contingency**: Maintain local song database backup

5. **Multi-Group Concurrency**
   - **Risk**: High concurrency may cause performance issues
   - **Mitigation**:
     - Use async/await throughout (FastAPI, Beanie, httpx)
     - Implement connection pooling for MongoDB
     - Load testing before production
   - **Contingency**: Scale horizontally with multiple backend instances

### Medium-Risk Items

1. **LangBot Integration Complexity**: Mitigate with thorough documentation review
2. **Beanie ODM Learning Curve**: Mitigate with examples and type hints
3. **Content Filtering Accuracy**: Mitigate with keyword + LLM hybrid approach
4. **Language Detection Edge Cases**: Mitigate with user preference override

## Dependencies

### External Services
- **taikowiki API**: Real-time song database (HTTP endpoint) - PRIMARY data source. Local JSON file (data/database.json) used ONLY as fallback when API unavailable
- **OpenRouter API**: gpt-4o access (REST API)
- **QQ Platform**: Via LangBot integration
- **MongoDB**: User data storage (self-hosted or cloud)

### Infrastructure (Must Run Continuously per NFR-013)
- **FastAPI Backend + Temporal Client**: Must run continuously to handle webhooks, call gpt-4o, and execute 5-step processing chain (Docker container in this project)
- **Temporal Server + PostgreSQL**: Must run continuously as workflow engine to schedule tasks and handle retries. Downtime prevents workflow execution (Docker containers in this project)
- **MongoDB**: Must run continuously to store user data, conversation history, and impressions. Downtime causes memory loss and data unavailability (Docker container in this project)
- **Nginx**: Strongly recommended for long-running reverse proxy with HTTPS and load balancing to provide stable public network entry point (Docker container in this project, optional but recommended)

### External Services (Must Run Continuously, Deployed Separately)
- **LangBot**: Must run continuously as core bot platform to receive QQ messages and manage triggers. Downtime prevents message reception. Deployed separately (not in this project's Docker Compose), connects to FastAPI backend via webhook at `/webhook/langbot`. For local development, webhook endpoint must be exposed using tunneling service (ngrok recommended). For production, use public domain/IP with Nginx reverse proxy (HTTPS recommended)
- **Napcat**: Must run continuously as QQ protocol layer to keep bot account online. Downtime prevents receiving messages from QQ platform. Deployed separately or as part of LangBot deployment

### Development Tools
- **Poetry**: Dependency management
- **pytest**: Testing framework with pytest-asyncio for async tests
- **pytest-cov**: Test coverage reporting (80%+ target)
- **pytest fixtures**: Test utilities for common test data and mocks (NFR-012)
- **Black**: Code formatting
- **Docker Compose**: Deployment orchestration

## Clarifications Integrated

This plan incorporates all clarifications from the specification:

- ‚úÖ **User Identification**: Hashed QQ user ID (SHA-256) for privacy-compliant cross-group recognition
- ‚úÖ **Data Retention**: 90-day auto-deletion for conversations, permanent retention for user preferences until explicit deletion
- ‚úÖ **Rate Limiting**: 20 requests/user/minute, 50 requests/group/minute with high-quality concurrent multi-threaded architecture
- ‚úÖ **Content Filtering**: Hybrid approach - keyword/phrase lists for fast pre-filtering, LLM judgment for ambiguous cases
- ‚úÖ **Language Detection**: Automatic detection with user preference override capability
- ‚úÖ **API Key Management**: All keys loaded from environment variables via config.py using os.getenv(), config files contain placeholder values only
- ‚úÖ **Structured Logging**: JSON format with structured fields (hashed user ID, request ID, timestamp, operation type, log level, message, contextual metadata), no sensitive data logging
- ‚úÖ **Monitoring**: Basic metrics (request rate, error rate, response time percentiles, system resources) via /health and /metrics endpoints
- ‚úÖ **Caching Strategy**: In-memory caching with periodic refresh (e.g., hourly) for song data, queries prioritize cached data
- ‚úÖ **Temporal Retry**: Exponential backoff (1s, 2s, 4s, 8s intervals) with maximum 5 retry attempts
- ‚úÖ **Image Processing**: 10MB size limit, JPEG/PNG/WebP formats only, user-friendly themed error messages ("ÂõæÁâáÂ§™Â§ßÂï¶ÔºÅ"), comprehensive prompt engineering
- ‚úÖ **Prompt Template System**: Structured template system with simple API (`add_prompt` function), organized by use case (general chat, song queries, image analysis, memory-aware), support for versioning and A/B testing (FR-013)
- ‚úÖ **Testing Infrastructure**: Comprehensive testing infrastructure per NFR-012 including: (1) Automated test suite (pytest) with unit tests and integration tests covering all user stories and edge cases (80%+ coverage target), (2) Test utilities (pytest fixtures) for common test data (mock users, conversations, songs) and mocks for external services (OpenRouter API, taikowiki, MongoDB), (3) Manual testing scripts in `scripts/` directory for quick functional verification (test_webhook.py, test_song_query.py)
- ‚úÖ **Deployment Architecture**: All infrastructure services must run continuously per NFR-013: FastAPI backend + Temporal client, Temporal server + PostgreSQL, MongoDB (Nginx strongly recommended). External services that must also run continuously: LangBot (deployed separately, connects via webhook at `/webhook/langbot`), Napcat (deployed separately or as part of LangBot). For local development, webhook endpoint must be exposed using tunneling service (ngrok recommended). For production, use public domain/IP with Nginx reverse proxy (HTTPS recommended). Docker Compose deployment must ensure all services start and remain running with health checks and restart policies. Any service downtime will cause functional failures. Note: LangBot and Napcat are external services deployed separately from this project
- ‚úÖ **User Preference Extraction**: LLM automatically analyzes conversation content and extracts user preferences. Before updating Impression, system asks user for confirmation in bot's response. System prioritizes explicit confirmation ("ÊòØ", "ÂØπ", "yes", "correct", etc.), with implicit confirmation as fallback if user continues conversation assuming preference is correct. Only after user confirms (explicitly or implicitly) should preferences be permanently stored (FR-010)
- ‚úÖ **Fuzzy Matching Strategy**: When multiple matches found, return only single best match (highest similarity score) and ask user for confirmation (e.g., "‰Ω†ÊòØÊåá„ÄäXXX„ÄãÂêóÔºü" / "Did you mean 'XXX'?") before providing song details. Confirmation request included in themed response with game elements per FR-003 (FR-004)
- ‚úÖ **Image Analysis Detail**: For Taiko no Tatsujin related images, comprehensive detailed analysis including song identification, difficulty level, score details, and other relevant game elements. For non-Taiko related images, themed response but politely indicate bot focuses on Taiko-related content. All image analysis responses maintain thematic consistency with game elements ("Don!", "Katsu!", emojis ü•Åüé∂) per FR-003 (FR-006)
- ‚úÖ **Conversation History Limit Configuration**: Support configuration via environment variable (e.g., `CONVERSATION_HISTORY_LIMIT`) with default value of 10. Allows future flexibility to adjust context window size based on performance requirements, LLM token limits, or user needs. Configuration loaded from config.py using environment variables per NFR-009 (FR-005)
- ‚úÖ **Song Database Primary Data Source**: taikowiki API is PRIMARY data source. System fetches from API first and updates local JSON file (data/database.json) periodically (hourly) for caching. Local JSON file used ONLY as fallback when API unavailable. When updating local file, system replaces existing data for consistency (FR-002 Enhancement)
- ‚úÖ **Concurrent Message Deduplication**: System processes messages in order but skips duplicate or highly similar messages. Similarity threshold and deduplication window (e.g., 5 seconds) configurable via environment variables (FR-008 Enhancement)
- ‚úÖ **Unconfirmed Preference Handling**: System retains unconfirmed preferences in pending state but does not actively re-ask. Re-confirms naturally when users mention related topics in future conversations (FR-010 Enhancement)
- ‚úÖ **Detailed Error Recovery**: System uses cached data or default themed responses with user notification when services fail. Song database falls back to local JSON file, LLM may support local models in future (FR-009 Enhancement)
- ‚úÖ **Intent Detection Fallback**: When intent detection fails or is uncertain, system falls back to use_case-based prompt selection and logs failures for improvement (FR-013 Enhancement)
- ‚úÖ **Intent Classification and Scenario-Based Prompting**: System supports fine-grained intent classification (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) and scenario-based prompts for specific contexts (e.g., "song_recommendation_high_bpm", "difficulty_advice_beginner"). Prompts organized hierarchically: use_case ‚Üí intent ‚Üí scenario. Intent detection uses rule-based keyword matching initially, with optional LLM-based classification for complex cases. This enables precise prompt selection and better response quality (FR-013 Enhancement)

## Next Steps

1. **Generate `research.md`**: Complete Phase 0 research tasks (all clarifications integrated)
2. **Generate `data-model.md`**: Detailed entity definitions
3. **Generate `contracts/`**: OpenAPI and Temporal definitions (updated with clarifications)
4. **Generate `quickstart.md`**: Setup and deployment guide
5. **Update agent context**: Run update script for Cursor
6. **Proceed to `/speckit.tasks`**: Break down into implementable tasks

---

**Status**: Phase 0 & Phase 1 Planning Complete (All Clarifications Integrated, Including Latest 5 Clarifications + Intent Classification Enhancement)  
**Latest Updates**: 
- User preference extraction with confirmation mechanism (explicit priority, implicit fallback)
- Fuzzy matching confirmation strategy (single best match + user confirmation)
- Image analysis detail requirements (comprehensive for Taiko images, themed for non-Taiko)
- Conversation history limit configuration (environment variable with default 10)
- User confirmation judgment standards (explicit vs. implicit)
- **Intent Classification and Scenario-Based Prompting**: Fine-grained intent classification (greeting, help, goodbye, preference_confirmation, song_query, song_recommendation, difficulty_advice, game_tips, etc.) and scenario-based prompts (e.g., "song_recommendation_high_bpm", "difficulty_advice_beginner"). Hierarchical organization: use_case ‚Üí intent ‚Üí scenario. Rule-based keyword matching with optional LLM-based classification

**Next Command**: `/speckit.tasks` to generate task breakdown
